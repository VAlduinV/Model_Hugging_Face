# COMPRASION.md

Порівняння результатів **Лабораторної №2** (класичний ML) та **Лабораторної №4** (Hugging Face pipelines).

---

## Дані / Умови експериментів

- **ЛР2**: тест — 4 000 прикладів (балансований), модель: *TF‑IDF (uni+bi, stopwords=en) + LogisticRegression (liblinear)*.  
- **ЛР4**: тест — 400 000 прикладів (великий збалансований корпус), моделі: DistilBERT та RoBERTa (SST‑2).

> Примітка: більший тест у ЛР4 забезпечує тісніші довірчі інтервали й менш «шумні» оцінки, але також ускладнює інференс на CPU.

---

## Порівняльні метрики

### ЛР2 (user baseline)
- **Accuracy:** 0.8595  
- **Клас 1 (NEGATIVE)**: P=0.87, R=0.84, F1=0.85, support=1951  
- **Клас 2 (POSITIVE)**: P=0.85, R=0.88, F1=0.87, support=2049  
- **Macro‑F1:** 0.8600

### ЛР4 (цей проєкт)
| Model                                           |   Accuracy |   NEG_P |   NEG_R |   NEG_F1 |   POS_P |   POS_R |   POS_F1 |   Support |
|:------------------------------------------------|-----------:|--------:|--------:|---------:|--------:|--------:|---------:|----------:|
| textattack/roberta-base-SST-2                   |     0.9422 |  0.9219 |  0.9661 |   0.9435 |  0.9644 |  0.9182 |   0.9407 |    400000 |
| distilbert-base-uncased-finetuned-sst-2-english |     0.8983 |  0.8804 |  0.9218 |   0.9006 |  0.918  |  0.8747 |   0.8958 |    400000 |

**Найкраща з двох**: **textattack/roberta-base-SST-2**  
- **Accuracy:** 0.9422  (Δ до ЛР2: +0.0827)  
- **Macro‑F1:** 0.9421 (Δ до ЛР2: +0.0821)

---

## Інтерпретація різниць

1. **Ядро представлення тексту.**  
   - *ЛР2:* Sparse‑ознаки TF‑IDF (уні+біграми) лінійно відокремлюють класи; добре працюють на експліцитній лексиці, слабше — на контексті/іронії/неявній полярності.  
   - *ЛР4:* Трансформери містять контекстні ембеддінги, що враховують порядок слів, заперечення, багатозначність; це дає **вищий recall і F1** на змішаних формулюваннях.

2. **Масштаб даних.**  
   - Оцінка на 400k зменшує «везіння/невезіння» вибірки і краще відображає реальну якість. Тому **приріст accuracy** у ЛР4 показує стійку перевагу контекстних моделей.

3. **Класовий баланс і симетрія помилок.**  
   - В обох роботах класи збалансовані; у ЛР2 метрики класів близькі → модель не перекошена.  
   - У ЛР4 RoBERTa зберігає симетрію, але піднімає **recall** для NEGATIVE та POSITIVE → менше пропусків «тонких» негативів/позитивів.

4. **Обчислювальна вартість.**  
   - *ЛР2:* дуже швидко на CPU, тренується/інференситься миттєво.  
   - *ЛР4:* інференс важчий; для 400k зразків бажано **GPU** (`--device 0`, `--batch 64–128`).

---

## Коли що обирати

- **TF‑IDF + LR (ЛР2)**
  - ✔ Малий бюджет ресурсів, швидкий бейзлайн, інтерпретовані ваги n‑грам.  
  - ✖ Слабше розуміє контекст/сарказм/заперечення.  
- **RoBERTa/DistilBERT (ЛР4)**
  - ✔ Вища якість на багатозначних формулюваннях; кращий *macro‑F1*.  
  - ✖ Важче, потребує GPU для великих партій.

---

## Що робити далі (узгоджено з рекомендаціями з ЛР2)

- Для **ЛР2‑підходу**: розширити `max_features`, увімкнути `sublinear_tf=True`, додати **char n‑grams**, спробувати `LinearSVC` — це дає +1…+2 п.п. на подібних корпусах.  
- Для **ЛР4‑підходу**: виконати **тонке донавчання** (`Trainer`/`peft LoRA`) на `train.ft.txt` (1–3 епохи, lr 2e‑5, max_len 256) — очікуваний приріст **ще 1–3 п.п.** accuracy над «ванільними» пайплайнами.  
- Для продакшену: зберігати **конфіг промптів/версії моделей/seed** і мати легку модель (DistilBERT) як fallback, важчу (RoBERTa) — як основну.

---

**Джерело метрик ЛР4:** `summary.json` з цього проєкту.
